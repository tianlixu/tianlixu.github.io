#+STARTUP: showall
#+TITLE: Notes for Neural Networks and Deep Learning
#+OPTIONS: \n:t
## #+STARTUP: indent

* NN and DL
- RELU function: Rectified Linear Unit
- A single neuron network: y = wx + b, wich is linear regression
- Supervised learning
- Structured data vs unstructured data
- Drivers behind the rise of Deep Learning
  Scale drives deep learning process, to hit a very high performance you need two things: 
  1. bigger NN to take advantage of huge amount of data
  2. more data

  Scale of:
  1. data
  2. computation: CPU GPU
  3. algorithms: sigmoid -> RELU function

- Notation:
  + (x, y) is an training example( x \in R^{n}, y \in {0, 1} )
  + m: the # of training examples( (x^{1}, y^{1}), (x^{2}, y^{2}), ..., (x^{m}, y^{m}) )
    + m_{train}: the # of training examples
    + m_{test}: the # of test examples
  + n_{x}: the dimention of the input feature vector
  + X: the input data
    X = 
    \begin{bmatrix} 
         |   |     | \\
         x^{1 } x^{2} ... x^{m} \\
         |   |     |        
    \end{bmatrix}    

    X \in R^{nxm}, X.shape = (n_{x}, m)

  + Y: the output
    Y = 
    \begin{bmatrix}
    y^{1} y^{2} ... y^{m}
    \end{bmatrix}

    Y \in R^{1xm}, Y.shape = (1, m)
  + Keep w and b as separate  parameters
  + superscript i indicates the i_{th} training example
  + Loss functon is applied for only one single training example
  + Cost function is for the entire training set

* Logistic Regression
   - Given x, want $\hat{y}$ = P(y=1|x)
     x \in R^{n}, 
   - Parameters: w \in R^{n}, b \in R
   - Output:
     
     $\hat{y}$ = w^{T}x + b, is a linear functon of input x
     
     $\hat{y}$ = \sigma(w^{T}x + b)
     
     \sigma(z) = $\frac{1}{1+e^{-z}}$, or \sigma(z) = $1\Big/(1+e^{-z})^{}$
     + \sigma(0) = 0.5
     + \sigma(\infty) = 1
     + \sigma(-\infty) = 0
** Cost function
   - $\hat{y^i}$ = \sigma(w^{T}x^{i} + b), where \sigma(z^{i}) = $\frac{1}{1+e^{(-z)^{i}}}$
   - Given {(x^{1}, y^{1}), ..., (x^{m}, y^{m})}, train the model and find w and b so that $\hat{y}^i$ \approx y^{i}
   - Loss (error) function
     + want loss function as small as possible
     + First function would be the squared root error $\mathcal{L}$($\hat{y}$, y) = $\frac{1}{2}$ ($\hat{y}$ - y)^{2}
       + we do not use this because it may lead to optimization problem which is not convex, means it contains
         local optimum points.
     + The loss function we'll use: $\mathcal{L}$($\hat{y}$, y) = -(ylog$\hat{y}$ +(1-y)log(1-$\hat{y}$))
       + if y=1 $\mathcal{L}$($\hat{y}$, y) = -log$\hat{y}$, want log$\hat{y}$ large, want $\hat{y}$ large(y->1)
       + if y=0 $\mathcal{L}$($\hat{y}$, y) = -log(1-$\hat{y}$), want log(1-$\hat{y}$) large, want $\hat{y}$ small(y->0)
       + the loss functiion was defined with respect to a single training example
   - Cost function which measures how well you are doing on the entire training set.
     + J(w, b) = $\frac{1}{m}$ $\sum_{i=1}^{m}$ L($\hat{y}^{i}$, y^{i}) = -$\frac{1}{m}$ $\sum_{i=1}^{m}$ [y^{i}log$\hat{y}^{i}$ + (1-y^{i})log(1-$\hat{y}^{i}$)]
** Gradient Descent
   want to find w,b that minimize $\mathcal{J}$(w,b), $\mathcal{J}$(w,b) is a convex function.
   $\frac{\partial{J}}{\partial{w}}$

   Repeat {

       w := w - \alpha$\frac{\partial{J(w)}}{\partial{w}}$ = w - \alpha*dw

   }
   
   - \alpha is the learning rate and it controls how big a step we take on each iteration.
   - use "dw" to represent derivative term $\frac{\partial{J(w)}}{\partial{w}}$, same for "db"
** Derivatives
   - f(a) = 3a, the slope(derivative) of f(a) at a=2 is 3.
     
     $\frac{df(a)}{da}$ = 3 = $\frac{d}{da}$ f(a)

   - J = 3v, v = a + u 
     
     $\frac{dJ}{du}$ = $\frac{dJ}{dv}$ $\frac{dv}{du}$ = 3 * 1 = 3
** Computation Graph
   - forward or left to right calculation to compute the cost function
   - backward or right to left calculation to compute the derivatives

** Logistic Regression Gradient descent
   x_{1},w_{1},x_{2},w_{2},b  ->  z = w_{1}x_{1} + w_{2}x_{2} + b  ->  a = \sigma(z)  -> L(a,y)
   - da = $\frac{dL}{da}$ = -$\frac{y}{a}$ + $\frac{1-y}{1-a}$
   - dz = $\frac{dL}{dz}$ = $\frac{dL}{da}$ * $\frac{da}{dz}$ = (-$\frac{y}{a}$ + $\frac{1-y}{1-a}$) * a(1-a) = a-y
   - dw_{1} = x_{1}*dz,  dw_{2} = x_{2}*dz,  db = dz
   - w_{1} := w1 -\alpha dw_{1},  w_{2} := w_{2} - \alpha dw_{2},  b := b - \alpha db
** Logistic Regression on m examples
#+BEGIN_VERSE
   J=0, dw1=0, dw2=0, db=0
   For i = 1 to m
       z^{i} = w^{T}x^{i} + b
       a^{i} = \sigma(z^{i})
       J += -(y^{i}loga^{i} + (1-y^{i})log(1-a^{i}))
       dz^{i} = a^{i} - y^{i}
       dw_{1} += x_{1}^{i} dz^{i}
       dw_{2} += x_{2}^{i} dz^{i}   # we have only 2 features in this example
       db += dz^{i}
   J /= m
   dw_{1} /= m, dw_{2} /= m, db /= m
   w_{1} := w1 -\alpha dw_{1},  w_{2} := w_{2} - \alpha dw_{2},  b := b - \alpha db
#+END_VERSE

   Note:
   - as you can see, implementation logistic regression in this way, you need to write 2 for-loops. One for-loop over m training examples and the second for-loop is a for-loop over all the n features.
   - two many explicit for-loops makes the algorithms run less efficiency. Use vertorization instead.

** Vectorization
   avoid using explicit for-loops.
   Single Instruction Multiple Data(SIMD) features in python

   dw_{1} = 0, dw_{2} = 0  --> dw = np.zeros((n_{x}, 1))

   dw_{1} += x_{1}^{i} dz^{i}
   dw_{2} += x_{2}^{i} dz^{i}    --> dw += x^{(i)}dz^{(i)}

   dw_{1} /= m, dw_{2} /= m    --> dw /= m   

   #+BEGIN_VERSE
   J=0, dw=0, db=0
   For i = 1 to m
       z^{i} = w^{T}x^{i} + b
       a^{i} = \sigma(z^{i})
       J += -(y^{i}loga^{i} + (1-y^{i})log(1-a^{i}))
       dz^{i} = a^{i} - y^{i}
       dw += x^{(i)}dz^{(i)}
       db += dz^{i}
   J /= m
   dw /= m, db_{} /= m
   w_{1} := w1 -\alpha dw_{1},  w_{2} := w_{2} - \alpha dw_{2},  b := b - \alpha db   
   #+END_VERSE
   
** Vectorizing Logistic Regression
Z = [z^{1}, z^{2}, ..., z^{m}] = w^{T} X + [b, b, ..., b] = [w^{T}x^{1} + b, w^{T}x^{2} + b, ..., w^{T}x^{m} + b]
#+BEGIN_SRC
Z = np.dot(w.T, X) + b   Note: b here is a (1,1) number
#+END_SRC

A = [a^{1}, a^{2}, ..., a^{m}] = \sigma(Z)

Y = [y^{1}, y^{2}, ..., y^{m}]

dZ = [dz^{1}, dz^{2}, ..., dz^{m}] = [a^{1}-y^{1}, a^{2}-y^{2}, ..., a^{m}-y^{m}] = A - Y

db = $\frac{1}{m}$ $\sum_{i=1}^{m}$ dz^{i} = $\frac{1}{m}$ np.sum(dZ)

dw = $\frac{1}{m}$ X dZ^{T}

#+BEGIN_VERSE
Z = w^{T}X + b = np.dot(w.T, X) + b
A = \sigma(Z)
dZ = A - Y
dw = $\frac{1}{m}$ X dZ^{T}    *X \in R^{nxm}    dZ \in R^{1xm}    dw \in R^{nx1}*
db = $\frac{1}{m}$ np.sum(dZ)

w := w - \alpha dw
b := b - \alpha db

#+END_VERSE

** Python/numpy vectors
- a = np.random.randn(5)
  + a.shape = (5,), is a "rank 1" array
  + Don't use this "rank 1" array
- a = np.random.randn(5, 1), a.shape = (5,1) is a volumn vector
- a = np.random.randn(1, 5), a.shape = (1,5) is a row vector
- use assertion to check the shape of a vector/matrix
  + assert(a.shape = (5,1))
- use reshape to reshape an array
  + a = a.reshape((5,1))
  
* Logistic regression cost function
** cost on one single example
Interpre $\hat{}$ = P(y=1|x)  The chance that y is equal to 1 for a given set of input features x.

If y=1    P(y|x) = $\hat{y}$         the chance of y is equal to 1
If y=0    P(y|x) = 1 - $\hat{y}$     the chance of y is equal to 0

->

P(y|x) = $\hat{y}^{y}$ $(1-\hat{y})^(1-y)$

->

logP(y|x) = log($\hat{y}^{y}$ $(1-\hat{y})^(1-y)$) = ylog$\hat{y}$ + (1-y)log(1-$\hat{y}$) = -L($\hat{y}$, y)
** Cost on m examples
P(lables in training set) = $\prod_{i=1}^{m}$ P(y^{(i)}|x^{(i)})
-> logP(lables in training set) = log $\prod_{i=1}^{m}$ P(y^{(i)}|x^{(i)})
-> logP(...) = $\sum_{i=1}^{m}$ logP(y^{(i)}|x^{(i)}) = -$\sum_{i=1}^{m}$ L($\hat{y}^{(i)}$, y^{(i)})
in statistics, it is called the principle of maximum likelihood estimation, which means choose parameters to maximum logP(...)
-> This justifies the cost we had for logistic regression
Cost: J(w,b) = $\sum_{i=1}^{m}$ L($\hat{y}^{(i)}$, y^{(i)})
because we now want to minimize the J(w,b) instead of maximum the likelihood, we've gotten rid of the minus sign in previous equation.
-> Finally, for convenience or the initial that our quantities are better scale, we just add one over m extra scaling factor there.
Cost: J(w,b) = $\frac{1}{m}$ $\sum_{i=1}^{m}$ L($\hat{y}^{(i)}$, y^{(i)})

To summarize, by minimizing this cost function J of w and b we're really carrying out maximum likelihood estimation with the logitic
regression model under the assumption that our training examples were on my ID or identically independently distributed.

* Neural Network Reresentation
** For one example
Given input x:
    + a^{[0]} = x
    + z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}
      a^{[1]} = \sigma(z^{[1]})
    + z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}
      a^{[2]} = \sigma(z^{[2]})
      
#+ATTR_HTML: :width 800
#+ATTR_ORG: :width 800
#+CAPTION: for-loop on one example
[[./img/dl_ang/nn_reprentation_1_example.png]]

#+ATTR_HTML: :width 800
#+ATTR_ORG: :width 800
#+CAPTION: Vectoring on one example
[[./img/dl_ang/nn_reprentation_1_example_2.png]]

** For m examples
#+ATTR_HTML: :width 800
#+ATTR_ORG: :width 800
#+CAPTION: for-loop on m examples
[[./img/dl_ang/nn_reprentation_m_examples_1.png]]

#+ATTR_HTML: :width 800
#+ATTR_ORG: :width 800
#+CAPTION: Vectorizing accross m examples
[[./img/dl_ang/nn_reprentation_m_examples_2.png]]

- Horizontal index: different training examples from left to right yor are scanning through the training set
  Vertical index : different nodes in the nueural network, 

#+ATTR_HTML: :width 800
#+ATTR_ORG: :width 800
#+CAPTION: Recap
[[./img/dl_ang/nn_reprentation_m_examples_recap.png]]

* Activation function
** g(Z)
   - g(Z) = sigmoid(Z) = $\frac{1}{1+e^{-Z}}$, \in (0,1)
   - g(Z) = tanh(Z) = $\frac{e^{Z} - e^{-Z}}{e^{Z} + e^{-Z}}$, \in (-1, 1)
     always works better than sigmoid function, the only exception is the output layer for a binary classification
     in which case y \in {0, 1}
   - one of the downsides of both sigmoid and tanh funcions is that if Z is either very large or very small
     then the gradient or the drivative of the slope of this funcions becomes very small(being closing to zero),
     which slows down gradient descent.
   - g(Z) = RELU(Z) = max(0, Z)
     This is mostly recommended for hidden layers.
     RELU: Rectified linear unit, the derivative is 1 as long as Z is positive and the derivative or the slope is
     zero when Z is negative.
   - g(Z) = leaky RELU(Z)
     Usually works better than RELU although it's not used as much in practice. Either one should be fine.
   - Neural network using RELU or leaky RELU learns faster than that using simoid and tanh.

   #+ATTR_HTML: :width 800
   #+ATTR_ORG: :width 800
   #+CAPTION: Recap of activation functions
   [[./img/dl_ang/nn_activation_functions.png]]

** Why non-linear activation functions?
   - A linear activation function: the neural network is just outputting a linear function of the input no matter how many layers your neural network has.
   - Output layer may be OK using linear function if you are doing a regression problem and y is a real number y \in R
   - But for hidden layers, never use linear activation fucntions, use RELU, tanh...

* Derivatives of activation functions
    a = g(z)
  - sigmoid function
    g'(z) = $\frac{d}{dz}$ g(z) = g(z)(1 - g(z)) = a(1-a)
  - tanh function
    g'(z) = 1 - (tanh(z))^{2} = 1 - a^{2}
  - ReLU
    g'(z) =
        + 0, if z < 0
        + 1, if z >= 0
 - Leaky ReLU
   g'(z) =
       + 0.01, if z < 0
       + 1,    if z >= 0

* Gradient descent for neural networks
  n^{[0]} = n_{x} : number of feature for input layer
  n^{[1]} : number of nodes in hidden layer
  n^{[2]} : number of nodes in output layer

**  Parameters:
  W^{[1]} : (n^{[1]}, n^{[0]}) every row of W represents the weights of the input in previous layer
  b^{[1]} : (n^{[1]}, 1) you can think of b as a value attached to each node
  W^{[2]} : (n^{[2]}, n^{[1]})
  b^{[2]} : (n^{[2]}, 1)

**  Cost fucntion:
  J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = $\frac{1}{m}$ $\sum_{i=1}^{m}$ L($\hat{y}$, y)
  $\hat{y}$ = a^{[2]}

**  Use gradient descent to train the parameters,
  Repeat {
      - compute predictions: $\hat{y}^{(i)}$ for i = 1,2,...,m
      - compute derivatives:
        dW^{[1]}, db^{[1]}, dW^{[2]}, db^{[2]}
        W^{[1]} := W^{[1]} - \alpha dW^{[1]}
        b^{[1]} := b^{[1]} - \alpha db^{[1]}
        W^{[2]} := W^{[2]} - \alpha dW^{[2]}
        b^{[2]} := b^{[2]} - \alpha db^{[2]}        
  }

** Formulas for computing derivatives
   - Forward propagation:
     Z^{[1]} = W^{[1]}X + b^{[1]}
     A^{[1]} = g^{[1]}(Z^{[1]})
     Z^{[2]} = W^{[2]}X + b^{[2]}
     A^{[2]} = g^{[2]}(Z^{[2]})
   - Back propagation:
     dZ^{[2]} = A^{[2]} - Y
     dW^{[2]} = $\frac{1}{m}$ dZ^{[2]} A^{[1]T}
     db^{[2]} = $\frac{1}{m}$ np.sum(dZ^{[2]}, axis=1, keepdims=True)  (n^{[2]}, 1)

     dZ^{[1]} = W^{[2]T} dZ^{[2]} * g'^{[1]}(Z^{[1]})
     dW^{[1]} = $\frac{1}{m}$ dZ^{[1]} A^{[0]T},  X = A^{[0]}
     db^{[1]} = $\frac{1}{m}$ np.sum(dZ^{[1]}, axis=1, keepdims=True)    (n^{[1]}, 1)
     
* Random Initialization
** what happens if you initialize weights to zero?
   a_{1}^{[1]} = a_{2}^{[1]}  dz_{1}^{[1]} = dz_{2}^{[1]}
   
  
    dW = 
    \begin{bmatrix}
        u,v \\
        u,v
    \end{bmatrix}    

    W^{[1]} := W^{[1]} - \alpha dW

    W = 
    \begin{bmatrix}
        ...... \\
        ......
    \end{bmatrix}
    The first row equals the second row.

    Both the hidden units compute the same function and both of them have the same influence on the output unit.
    Then after one interation the same statement is still true since the two hidden units are still symmetric.
    No matter after how many interations, no matter how long you train your network, both hidden units are still
    computing exactly the same function. In this case, there is no point to have more than one hidden unit because
    they are all computing the same thing. This is still true for large scale neural networks.
** Solution
   The solution is to initialize your parameters randomly.
   W^{[1]} = np.random.randn((2,2)) * 0.01
   b^{[1]} = np.zero((2,1))   # it is OK to set b as zeros
   W^{[2]} = np.random.randn((1,2)) * 0.01
   b^{[2]} = np.zero((1,1))

   why const 0.01 in the above initialization? why no 100, 1000?
   Usually prefer to initialize weights to get very small random values. Why?
   For tanh or sigmoid function, if weights are too large, you get very large activation values Z. So the derivatives are small and the gradient
   descent would be very slow, so the learning  would be very slow.

   If you don't have any tanh or sigmoid activiation function throughout your neural network this is less of an
   issue. But if you're doing binary classification, your ouput unit is a sigmoid function, you don't wany your
   initial prarameters to be too large.

   You may pick 0.001 or something else for a deep neural network.

* Deep neural network   
** Deep neural network notation
  L = 4 : # of layers
  n^{[l]}: # of units in layer l, n^{[0]} = n_{x}
  a^{[l]}: activiations in layer l, a^{[l]} = g^{[l]}(Z^{[l]})
  W^{[l]}: weights for Z^{[l]}
  b^{[l]}: bias of units in layer l
  X = a^{[0]}, $\hat{y}$ = a^{[L]}
** Forward propagation
   For a single training example, the equations are:
       - z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}
       - a^{[l]} = g^{[l]}(z^{[l]})

   Vectorizing for the whole training set:
   for l=1,2,...,L
       - Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}
       - A^{[l]} = g^{[l]}(Z^{[l]})   
  
** Matrix dimensions
   For a single training example:
   - z^{[l]}, a^{[l]}: (n^{[l]}, 1)
   - dz^{[l]}, da^{[l]}: the same dimension as z^{[l]}, a^{[l]}     
   - W^{[l]}: (n^{[l]}, n^{[l-1]})
   - b^{[l]}: (n^{[l]}, 1)
   - dW^{[l]}: the same dimension as W^{[l]}
   - db^{[l]}: the same dimension as b^{[l]}

   For m examples:
   - Z^{[l]}, A^{[l]}: (n^{[l]}, m)
   - dZ^{[l]}, dA^{[l]}: the same dimension as Z^{[l]}, A^{[l]}
   - W^{[l]}: (n^{[l]}, n^{[l-1]})
   - b^{[l]}: (n^{[l]}, m)
   - dW^{[l]}: the same dimension as W^{[l]}
   - db^{[l]}: the same dimension as b^{[l]}     

   
* Building blocks of a Deep Neural Network
  Basic build blocks to build a neural network in each layer there's:
  - a forward progation step
  - a corresponding back propagation step
  - a cache to save information from one to the other
  
  #+ATTR_HTML: :width 800
  #+ATTR_ORG: :width 800
  #+CAPTION: Forward and backward functions
  [[./img/dl_ang/forward_backward_functions.png]]

** Forward and backward propagation for layer l
  #+ATTR_HTML: :width 800
  #+ATTR_ORG: :width 800
  #+CAPTION: Backward propagation for layer l
  [[./img/dl_ang/backward_propagation_for_layer_l.png]]
  
  #+ATTR_HTML: :width 800
  #+ATTR_ORG: :width 800
  #+CAPTION: Backward propagation for layer l
  [[./img/dl_ang/backward_propagation_for_layer_l_summary.png]]  
  
* Parameters and Hyperparameters
** Parameters
   W and b
** Hyperparameters
   - learning rate \alpha
   - # interfations
   - # hidden layer L
   - # hidden units n^{[1]}, n^{[2]}, ... n^{[L]}
   - choice of activation functions
   - Momentum
   - minibatch size
   - regularizations
** Applied deep learning
   Applied deep learning is a very empirical process.
   Idea -> Code -> Experiment -> Idea ....
