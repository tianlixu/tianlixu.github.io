#+STARTUP: showall
#+TITLE: Notes for Neural Networks and Deep Learning

* Courses
1. Neural Networks and Deep Learning (4 weeks)
2. Improving Deep Neural Networks: Hyperameter tuning, Regularization and Optimization (3 weeks)
3. Structuring your Machine Learning project (2 weeks)
4. Convolutional Neural Networks(CNN)
5. Natural Language Processing(NLP): Builing Sequence Models

* NN and DL
- RELU function: Rectified Linear Unit
- A single neuron network: y = wx + b, wich is linear regression
- Supervised learning
- Structured data vs unstructured data
- Drivers behind the rise of Deep Learning
  Scale drives deep learning process, to hit a very high performance you need two things: 
  1. bigger NN to take advantage of huge amount of data
  2. more data

  Scale of:
  1. data
  2. computation: CPU GPU
  3. algorithms: sigmoid -> RELU function

- Notation:
  + (x, y) is an training example( x \in R^{n}, y \in {0, 1} )
  + m: the # of training examples( (x^{1}, y^{1}), (x^{2}, y^{2}), ..., (x^{m}, y^{m}) )
      + m_{train}: the # of training examples
      + m_{test}: the # of test examples
  + n: the dimention of the input feature vector
  + X: the input data

    X = 
    \begin{bmatrix} 
         |   |     | \\
         x^{1 } x^{2} ... x^{m} \\
         |   |     |        
    \end{bmatrix}    

    X \in R^{mxn}





Example: to be deleted
If $a^2=b$ and \( b=2 \), then the solution must be
either $$ a=+\sqrt{2} $$ or \[ a=-\sqrt{2} \].

\begin{matrix}
 a & b \\
 c & d \\
\end{matrix}

\begin{pmatrix} 
  \alpha     & \beta^{*}\\ 
  \gamma^{*} & \delta 
\end{pmatrix}

\begin{bmatrix} 
  \alpha     & \beta^{*}\\ 
  \gamma^{*} & \delta 
\end{bmatrix}
