#+STARTUP: showall
#+TITLE: Notes for Neural Networks and Deep Learning
#+OPTIONS: \n:t
#+STARTUP: indent

* Courses
1. Neural Networks and Deep Learning (4 weeks)
2. Improving Deep Neural Networks: Hyperameter tuning, Regularization and Optimization (3 weeks)
3. Structuring your Machine Learning project (2 weeks)
4. Convolutional Neural Networks(CNN)
5. Natural Language Processing(NLP): Builing Sequence Models

* NN and DL
- RELU function: Rectified Linear Unit
- A single neuron network: y = wx + b, wich is linear regression
- Supervised learning
- Structured data vs unstructured data
- Drivers behind the rise of Deep Learning
  Scale drives deep learning process, to hit a very high performance you need two things: 
  1. bigger NN to take advantage of huge amount of data
  2. more data

  Scale of:
  1. data
  2. computation: CPU GPU
  3. algorithms: sigmoid -> RELU function

- Notation:
  + (x, y) is an training example( x \in R^{n}, y \in {0, 1} )
  + m: the # of training examples( (x^{1}, y^{1}), (x^{2}, y^{2}), ..., (x^{m}, y^{m}) )
    + m_{train}: the # of training examples
    + m_{test}: the # of test examples
  + n_{x}: the dimention of the input feature vector
  + X: the input data
    X = 
    \begin{bmatrix} 
         |   |     | \\
         x^{1 } x^{2} ... x^{m} \\
         |   |     |        
    \end{bmatrix}    

    X \in R^{nxm}, X.shape = (n_{x}, m)

  + Y: the output
    Y = 
    \begin{bmatrix}
    y^{1} y^{2} ... y^{m}
    \end{bmatrix}

    Y \in R^{1xm}, Y.shape = (1, m)
  + Keep w and b as separate  parameters
  + superscript i indicates the i_{th} training example
  + Loss functon is applied for only one single training example
  + Cost function is for the entire training set

** Logistic Regression
   - Given x, want $\hat{y}$ = P(y=1|x)
     x \in R^{n}, 
   - Parameters: w \in R^{n}, b \in R
   - Output:
     
     $\hat{y}$ = w^{T}x + b, is a linear functon of input x
     
     $\hat{y}$ = \sigma(w^{T}x + b)
     
     \sigma(z) = $\frac{1}{1+e^{-z}}$, or \sigma(z) = $1\Big/(1+e^{-z})^{}$
     + \sigma(0) = 0.5
     + \sigma(\infty) = 1
     + \sigma(-\infty) = 0
** Cost function
   - $\hat{y^i}$ = \sigma(w^{T}x^{i} + b), where \sigma(z^{i}) = $\frac{1}{1+e^{(-z)^{i}}}$
   - Given {(x^{1}, y^{1}), ..., (x^{m}, y^{m})}, train the model and find w and b so that $\hat{y}^i$ \approx y^{i}
   - Loss (error) function
     + want loss function as small as possible
     + First function would be the squared root error $\mathcal{L}$($\hat{y}$, y) = $\frac{1}{2}$ ($\hat{y}$ - y)^{2}
       + we do not use this because it may lead to optimization problem which is not convex, means it contains
         local optimum points.
     + The loss function we'll use: $\mathcal{L}$($\hat{y}$, y) = -(ylog$\hat{y}$ +(1-y)log(1-$\hat{y}$))
       + if y=1 $\mathcal{L}$($\hat{y}$, y) = -log$\hat{y}$, want log$\hat{y}$ large, want $\hat{y}$ large(y->1)
       + if y=0 $\mathcal{L}$($\hat{y}$, y) = -log(1-$\hat{y}$), want log(1-$\hat{y}$) large, want $\hat{y}$ small(y->0)
       + the loss functiion was defined with respect to a single training example
   - Cost function which measures how well you are doing on the entire training set.
     + J(w, b) = $\frac{1}{m}$ $\sum_{i=1}^{m}$ L($\hat{y}^{i}$, y^{i}) = -$\frac{1}{m}$ $\sum_{i=1}^{m}$ [y^{i}log$\hat{y}^{i}$ + (1-y^{i})log(1-log$\hat{y}^{i}$)]
** Gradient Decent
   want to find w,b that minimize $\mathcal{J}$(w,b), $\mathcal{J}$(w,b) is a convex function.
   $\frac{\partial{J}}{\partial{w}}$

   Repeat {

       w := w - \alpha$\frac{\partial{J(w)}}{\partial{w}}$ = w - \alpha*dw

   }
   
   - \alpha is the learning rate and it controls how big a step we take on each iteration.
   - use "dw" to represent derivative term $\frac{\partial{J(w)}}{\partial{w}}$, same for "db"
** Derivatives
   - f(a) = 3a, the slope(derivative) of f(a) at a=2 is 3.
     
     $\frac{df(a)}{da}$ = 3 = $\frac{d}{da}$ f(a)

   - J = 3v, v = a + u 
     
     $\frac{dJ}{du}$ = $\frac{dJ}{dv}$ $\frac{dv}{du}$ = 3 * 1 = 3
** Computation Graph
   - forward or left to right calculation to compute the cost function
   - backward or right to left calculation to compute the derivatives

** Logistic Regression Gradient decent
   x_{1},w_{1},x_{2},w_{2},b  ->  z = w_{1}x_{1} + w_{2}x_{2} + b  ->  a = \sigma(z)  -> L(a,y)
   - da = $\frac{dL}{da}$ = -$\frac{y}{a}$ + $\frac{1-y}{1-a}$
   - dz = $\frac{dL}{dz}$ = $\frac{dL}{da}$ * $\frac{da}{dz}$ = (-$\frac{y}{a}$ + $\frac{1-y}{1-a}$) * a(1-a) = a-y
   - dw_{1} = x_{1}*dz,  dw_{2} = x_{2}*dz,  db = dz
   - w_{1} := w1 -\alpha dw_{1},  w_{2} := w_{2} - \alpha dw_{2},  b := b - \alpha db
** Logistic Regression on m examples
#+BEGIN_VERSE
   J=0, dw1=0, dw2=0, db=0
   For i = 1 to m
       z^{i} = w^{T}x^{i} + b
       a^{i} = \sigma(z^{i})
       J += -(y^{i}loga^{i} + (1-y^{i})log(1-a^{i}))
       dz^{i} = a^{i} - y^{i}
       dw_{1} += x_{1}^{i} dz^{i}
       dw_{2} += x_{2}^{i} dz^{i}   # we have only 2 features in this example
       db += dz^{i}
   J /= m
   dw_{1} /= m, dw_{2} /= m, db_{} /= m
   w_{1} := w1 -\alpha dw_{1},  w_{2} := w_{2} - \alpha dw_{2},  b := b - \alpha db
#+END_VERSE

   Note:
   - as you can see, implementation logistic regression in this way, you need to write 2 for-loops. One for-loop over m training examples and the second for-loop is a for-loop over all the n features.
   - two many explicit for-loops makes the algorithms run less efficiency. Use vertorization instead.

** Vectorization
   avoid using explicit for-loops.
   Single Instruction Multiple Data(SIMD) features in python

   dw_{1} = 0, dw_{2} = 0  --> dw = np.zeros((n_{x}, 1))

   dw_{1} += x_{1}^{i} dz^{i}
   dw_{2} += x_{2}^{i} dz^{i}    --> dw += x^{(i)}^{}dz^{(i)}

   dw_{1} /= m, dw_{2} /= m    --> dw /= m   

   #+BEGIN_VERSE
   J=0, dw=0, db=0
   For i = 1 to m
       z^{i} = w^{T}x^{i} + b
       a^{i} = \sigma(z^{i})
       J += -(y^{i}loga^{i} + (1-y^{i})log(1-a^{i}))
       dz^{i} = a^{i} - y^{i}
       dw += x^{(i)}^{}dz^{(i) }      
       db += dz^{i}
   J /= m
   dw /= m, db_{} /= m
   w_{1} := w1 -\alpha dw_{1},  w_{2} := w_{2} - \alpha dw_{2},  b := b - \alpha db   
   #+END_VERSE
   
*** Vectorizing Logistic Regression
Z = [z^{1}, z^{2}, ..., z^{m}] = w^{T} X + [b, b, ..., b] = [w^{T}x^{1} + b, w^{T}x^{2} + b, ..., w^{T}x^{m} + b]
#+BEGIN_SRC
Z = np.dot(w.T, X) + b   Note: b here is a (1,1) number
#+END_SRC

A = [a^{1}, a^{2}, ..., a^{m}] = \sigma(Z)

Y = [y^{1}, y^{2}, ..., y^{3}]

dZ = [dz^{1}, dz^{2}, ..., dz^{m}] = [a^{1}-y^{1}, a^{2}-y^{2}, ..., a^{m}-y^{m}] = A - Y

db = $\frac{1}{m}$ $\sum_{i=1}^{m}$ dz^{i} = $\frac{1}{m}$ np.sum(dZ)

dw = $\frac{1}{m}$ X dZ^{T}

#+BEGIN_VERSE
Z = w^{T}X + b = np.dot(w.T, X) + b
A = \sigma(Z)
dZ = A - Y
dw = $\frac{1}{m}$ X dZ^{T}    *X \in R^{nxm}    dZ \in R^{1xm}    dw \in R^{nx1}*
db = $\frac{1}{m}$ np.sum(dZ)

w := w - \alpha dw
b := b - \alpha db

#+END_VERSE

** Python/numpy vectors
- a = np.random.randn(5)
  + a.shape = (5,), is a "rank 1" array
  + Don't use this "rank 1" array
- a = np.random.randn(5, 1), a.shape = (5,1) is a volumn vector
- a = np.random.randn(1, 5), a.shape = (1,5) is a row vector
- use assertion to check the shape of a vector/matrix
  + assert(a.shape = (5,1))
- use reshape to reshape an array
  + a = a.reshape((5,1))
  
** Logistic regression cost function
Interpre $\hat{}$ = P(y=1|x)  The chance that y is equal to 1 for a given set of input features x.

If y=1    P(y|x) = $\hat{y}$         the chance of y is equal to 1
If y=0    P(y|x) = 1 - $\hat{y}$     the chance of y is equal to 0

->

P(y|x) = $\hat{y}^{y}$ $(1-\hat{y})^(1-y)$

->

logP(y|x) = log($\hat{y}^{y}$ $(1-\hat{y})^(1-y)$) = ylog$\hat{y}$ + (1-y)log(1-$\hat{y}$) = -L($\hat{y}$, y)

** test

