#+STARTUP: showall
#+TITLE: Notes for Neural Networks and Deep Learning

* Courses
1. Neural Networks and Deep Learning (4 weeks)
2. Improving Deep Neural Networks: Hyperameter tuning, Regularization and Optimization (3 weeks)
3. Structuring your Machine Learning project (2 weeks)
4. Convolutional Neural Networks(CNN)
5. Natural Language Processing(NLP): Builing Sequence Models

* NN and DL
- RELU function: Rectified Linear Unit
- A single neuron network: y = wx + b, wich is linear regression
- Supervised learning
- Structured data vs unstructured data
- Drivers behind the rise of Deep Learning
  Scale drives deep learning process, to hit a very high performance you need two things: 
  1. bigger NN to take advantage of huge amount of data
  2. more data

  Scale of:
  1. data
  2. computation: CPU GPU
  3. algorithms: sigmoid -> RELU function

- Notation:
  + (x, y) is an training example( x \in R^{n}, y \in {0, 1} )
  + m: the # of training examples( (x^{1}, y^{1}), (x^{2}, y^{2}), ..., (x^{m}, y^{m}) )
    + m_{train}: the # of training examples
    + m_{test}: the # of test examples
  + n_{x}: the dimention of the input feature vector
  + X: the input data
    X = 
    \begin{bmatrix} 
         |   |     | \\
         x^{1 } x^{2} ... x^{m} \\
         |   |     |        
    \end{bmatrix}    

    X \in R^{nxm}, X.shape = (n_{x}, m)

  + Y: the output
    Y = 
    \begin{bmatrix}
    y^{1} y^{2} ... y^{m}
    \end{bmatrix}

    Y \in R^{1xm}, Y.shape = (1, m)
  + Keep w and b as separate  parameters
  + superscript i indicates the i_{th} training example
  + Loss functon is applied for only one single training example
  + Cost function is for the entire training set

** Logistic Regression
   - Given x, want $\hat{y}$ = P(y=1|x)
     x \in R^{n}, 
   - Parameters: w \in R^{n}, b \in R
   - Output:
     
     $\hat{y}$ = w^{T}x + b, is a linear functon of input x
     
     $\hat{y}$ = \sigma(w^{T}x + b)
     
     \sigma(z) = $\frac{1}{1+e^{-z}}$, or \sigma(z) = $1\Big/(1+e^{-z})^{}$
     + \sigma(0) = 0.5
     + \sigma(\infty) = 1
     + \sigma(-\infty) = 0
** Cost function
   - $\hat{y^i}$ = \sigma(w^{T}x^{i} + b), where \sigma(z^{i}) = $\frac{1}{1+e^{(-z)^{i}}}$
   - Given {(x^{1}, y^{1}), ..., (x^{m}, y^{m})}, train the model and find w and b so that $\hat{y}^i$ \approx y^{i}
   - Loss (error) function
     + want loss function as small as possible
     + First function would be the squared root error $\mathcal{L}$($\hat{y}$, y) = $\frac{1}{2}$ ($\hat{y}$ - y)^{2}
       + we do not use this because it may lead to optimization problem which is not convex, means it contains
         local optimum points.
     + The loss function we'll use: $\mathcal{L}$($\hat{y}$, y) = -(ylog$\hat{y}$ +(1-y)log(1-$\hat{y}$))
       + if y=1 $\mathcal{L}$($\hat{y}$, y) = -log$\hat{y}$, want log$\hat{y}$ large, want $\hat{y}$ large(y->1)
       + if y=0 $\mathcal{L}$($\hat{y}$, y) = -log(1-$\hat{y}$), want log(1-$\hat{y}$) large, want $\hat{y}$ small(y->0)
       + the loss functiion was defined with respect to a single training example
   - Cost function which measures how well you are doing on the entire training set.
     + J(w, b) = $\frac{1}{m}$ $\sum_{i=1}^{m}$ L($\hat{y}^{i}$, y^{i}) = -$\frac{1}{m}$ $\sum_{i=1}^{m}$ [y^{i}log$\hat{y}^{i}$ + (1-y^{i})log(1-log$\hat{y}^{i}$)]
** Gradient Decent
   want to find w,b that minimize $\mathcal{J}$(w,b), $\mathcal{J}$(w,b) is a convex function.
   $\frac{\partial{J}}{\partial{w}}$

   Repeat {

       w := w - \alpha$\frac{\partial{J(w)}}{\partial{w}}$ = w - \alpha*dw

   }
   
   - \alpha is the learning rate and it controls how big a step we take on each iteration.
   - use "dw" to represent derivative term $\frac{\partial{J(w)}}{\partial{w}}$, same for "db"
** Derivatives
   - f(a) = 3a, the slope(derivative) of f(a) at a=2 is 3.
     
     $\frac{df(a)}{da}$ = 3 = $\frac{d}{da}$ f(a)

   - J = 3v, v = a + u 
     
     $\frac{dJ}{du}$ = $\frac{dJ}{dv}$ $\frac{dv}{du}$ = 3 * 1 = 3
** Computation Graph
   - forward or left to right calculation to compute the cost function
   - backward or right to left calculation to compute the derivatives

** Logistic Regression Gradient decent
