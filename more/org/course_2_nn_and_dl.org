#+STARTUP: showall
#+TITLE: Improving Deep Neural Networks
#+OPTIONS: \n:t

* Seting up ML application
** Data Sets
  Train/Dev/Test sets
  - Training set
    Use training set on your algorithms
  - Dev - Development set/ Cross Validation set
    Use Dev set to see which of many different models perform best on your dev set, then you get your final model
  - Test set
    Evaluate your model on your test set to get an unbiased estimate of how well your algorithm is doing

*** Ratio
   - Small amount of data(100, 1k, 10k), 60%, 20%, and 20% respectively
   - Big data era(> 1M), dev/test are much smaller
     98%, 1%, 1%, or 99.5%, 0.25%, 0.25, or 99.5%, 0.4%, 0.1%

*** Mismatched tain/test disriibution
   Rule of thumb:
   - Make sure dev and test sets come from the same distribution.
   - Your training set might come from a different distribution.
   - You might be OK to not have a test set

** Bias/Variance
   - High bias: Underfitting the data
   - High variance: Overfitting the data

    Looking at the training set error and dev set error, you'll render a diagnosis of your algorithm having
    high variance or high bias.

    e.g. for high variance:
    - train set error: 1%
    - dev set error:  10%
           
    e.g. for high bias: not even fit the training set very well
    - train set error: 15%
    - dev set error:   16%

    e.g. for high bias and high variance
    - train set error: 15%
    - dev set error:   30%

    e.g. for low bias and low variance
    - train set error: 0.5%
    - dev set error:   1% 

    Taking a look at your training error, you know how well you are fitting the training data, which tells you
    if you have a bias problem.

    Looking at how much higher your error goes(dev set error - train set error), that gives you a sens how bad
    is the variance problems.

** Basic recipe for ML
   high biase ? Y -> resolve it
   N -> high variance? -> resolve it -> high bias? -> Y -> go back
   N -> Done.
   
*** High bias
   Training data problem, try
   - bigger network(more layers/units)
   - train longer
   - Maybe NN architecture search
*** High variance
   Dev set problem, try
   - more data
   - regularization
   - Maybe NN architecture search

  
* Regularization
   \lambda is the regularization parameter  
** Logistic regression
*** L2 Regularization
     + J(w, b) = $\frac{1}{m}$ $\sum_{i=1}^{m}$ L($\hat{y}^{i}$, y^{i}) + $\frac{\lambda}{2m}$ ||w||^{2}_{2}
     + ||w||^{2}_{2} = $\sum_{j=1}^{n}$ w_{j}^{2} = w^{T}w
*** L1 Regularization
     + $\frac{\lambda}{2m} \sum_{j=1}^{n}|w|_{1}$ = $\frac{\lambda}{2m}$ ||w||_{1}
   
** Neural Network
     + J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]}) = $\frac{1}{m}$ $\sum_{i=1}^{m}$ L($\hat{y}^{i}$, y^{i}) + $\frac{\lambda}{2m} \sum_{l=1}^{L}  \|w^{[l]}\|^{2}_{2}$
     + $\|w^{[l]}\|^{2}_{F}$ = $\sum_{i=1}^{n^{[l-1]}} \sum_{j=1}^{n^{[l]}} (w_{ij}^{[l]})^{2}$
       w: (n^{[l-1]}, n^{[l]})
       Frobenius norm
     + dw^{[l]} = (from backprop) + $\frac{\lambda}{m}$ w^{[l]}
       w^{[l]} := w^{[l]} - \alpha dw^{[l]}
                = w^{[l]} - \alpha [(from backprop) + $\frac{\lambda}{m}$ w^{[l]}]
                = w^{[l]} -  \alpha $\frac{\lambda}{m}$ w^{[l]} - \alpha (from backprop)
                = (1 - $\frac{\alpha\lambda}{m}$)w^{[l]} - \alpha (from backprop)

       L2 regrulariztion is sometimes called "Weight decay"

** How does regularization prevent overfitting?
*** Intuition 1
   Cranking up \lambda to be very big will set W close to 0 - you can think of it as zeroing out or at least reducing
   the impact of hidden units, so you end up with a much simpler network, as if you were just using logistic regression,
   which prevents overfitting.
*** Intuition 2
    For example, we are using tanh activation function.
    If \lambda is large, then W^{[l]} is small, then Z^{[l]} is small - it takes on a small range of values, g(z)
    would be roughly linear, as if every layer would be roughtly linear, as if it's just logistic regression,
    then the whole network is jsut a linera network, which prevents overfitting.

** Dropout regularization
   - Drop some nodes, you get a diminished/reduced network.
   - If your deep NN is significantly overfitting, dropout will usually reduce the number of errors by a lot.
*** Implementation with Inverted dropout
    Illustrate with layer l=3, keep-prob=0.8
    - d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep-prob
      d3 is a bollean vector
      a3 = np.multiply(a3, d3) or a3 *= d3
      a3 /= keep-prob # scale a3 up, not to change the expected value of a3
    - No dropout at test time
    - No dropout for input layer
    - Use lower keep-prob for the layers have more hidden units, vice versa.
*** Why drop-out work
    Intuition: Can't rely on any one feature, so have to spread out weights.
    The input of a unit can get randomly eliminated.

    Use different keep-prob for different layer.

*** Downside
    The cost function J is no longer well defined on every iteration since it randomly killing off
    a bunch of nodes. What to do is turning off dropout by setting keep-pro = 1 and make sure that
    it is monotonically decreasing J and then turn on dropout.
   
** Other Regularization Methods
   - flip/rotate an image to get more brand new training examples
   - early-stopping, using dev set error, dev set usually goes down and then goes up, stop the training half way.


* Normalizing Inputs     
  Make features in the same scale so that it makes gradient decent faster.
  The cost function well on average look more symmetric and you have more spherical contours, you can have
  bigger learning rate, the gradient decent can pretty much go straight to the minimum you can take much larger
  steps. Makes cost function J easier and faster to optimize.

  - Step 1 subtract mean:
    u = $\frac{1}{m} \sum_{i=1}^{m}x^{i}$
    x := x - u
    move the training set until it has zero mean
  - Step 2 normalize variance:
    $\sigma^{2} = \frac{1}{m} \sum_{i=1}^{m}x^{i}**2$    element-wise squared
    x /= $\sigma^{2}$

  - Use normalization to scale your training data and *the same u and $\sigma^{2}$* to normalize your test set.
  - Use normalization when input features have dramaticlly different ranges.


* Vanishing/exploding gradients
  If L is large for very deep neural network, $\hat{Y}$ will be very large/small in fact that this grows/decreases
  exponentially. It makes gradient decent slow to learn anything. The deep NN suffers from this.

** Weight initialization for deep networks
     A partial solution which helps a lot on the problem of vanishing and exploding is careful choice of how you initialize the weights.
     - RELU
       W^{[l]} = np.random.randn(shape..) * np.sqrt($\frac{2}{n^{[l-1]}}$)
     - tanh
       sqrt($\frac{1}{n^{[l-1]}}$)  #Xavier initialization
       other variance:
       sqrt($\frac{2}{n^{[l-1]}*n^{[l]}}$)

    Choosing a reasonable scaling for how you initialize your weights which makes your weights not explode too quickly and not decay to
    zero too quickly, so that you can train a reasonaly deep nework without the weights or the gradients exploding or vanishing too much.
    
* Gradient Checking(Grad Check)
  Take W^{[1]}, b^{[1]}, ..., W^{[L]},b^{[L]} and reshape into a big vector \theta.
  J(W^{[1]}, b^{[1]}, ..., W^{[L]},b^{[L]) = J(\Theta) = J(\theta_{1}, \theta_{2}, \theta_{3}, ...)
  Take dW^{[1]}, db^{[1]}, ..., dW^{[L]},db^{[L]} and reshape into a big vector d\theta.

  for each i:
      d\Theta_{approx}[i] = $\frac{J(\theta_{1}, \theta_{2}, ..., \theta_{i}+\epsilon, ...) - J(\theta_{1}, \theta_{2}, ..., \theta_{i}-\epsilon, ...)}{2\epsilon}$ $\approx$ d\theta[i] = $\frac{dJ}{d\theta_{i}}$

  check d\Theta_{approx} \approx d\Theta   --> suppose \epsilon = 10^{-7}
  check $\frac{\|d\Theta_{approx} - d\Theta\|_{2}}{\|d\Theta_{approx}\|_{2} + \|d\Theta\|_{2}} \approx 10^{-7}$ is great, 10^{-5} OK, 10^{-3} worry
  
  Notes:
  - Don't use in training, only to debug, because computing d\Theta[i] is slow
  - If algorithm fails grad check, look at components to try to identify bug
  - Remember regularization
  - Doesn't work with dropout
  - Run at random initialization; perhaps again after some training
