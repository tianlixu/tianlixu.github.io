#+STARTUP: showall
#+TITLE: Improving Deep Neural Networks
#+OPTIONS: \n:t

* Seting up ML application
** Data Sets
  Train/Dev/Test sets
  - Training set
    Use training set on your algorithms
  - Dev - Development set/ Cross Validation set
    Use Dev set to see which of many different models perform best on your dev set, then you get your final model
  - Test set
    Evaluate your model on your test set to get an unbiased estimate of how well your algorithm is doing

*** Ratio
   - Small amount of data(100, 1k, 10k), 60%, 20%, and 20% respectively
   - Big data era(> 1M), dev/test are much smaller
     98%, 1%, 1%, or 99.5%, 0.25%, 0.25, or 99.5%, 0.4%, 0.1%

*** Mismatched tain/test disriibution
   Rule of thumb:
   - Make sure dev and test sets come from the same distribution.
   - Your training set might come from a different distribution.
   - You might be OK to not have a test set

** Bias/Variance
   - High bias: Underfitting the data
   - High variance: Overfitting the data

    Looking at the training set error and dev set error, you'll render a diagnosis of your algorithm having
    high variance or high bias.

    e.g. for high variance:
    - train set error: 1%
    - dev set error:  10%
           
    e.g. for high bias: not even fit the training set very well
    - train set error: 15%
    - dev set error:   16%

    e.g. for high bias and high variance
    - train set error: 15%
    - dev set error:   30%

    e.g. for low bias and low variance
    - train set error: 0.5%
    - dev set error:   1% 

    Taking a look at your training error, you know how well you are fitting the training data, which tells you
    if you have a bias problem.

    Looking at how much higher your error goes(dev set error - train set error), that gives you a sens how bad
    is the variance problems.

** Basic recipe for ML
   high biase ? Y -> resolve it
   N -> high variance? -> resolve it -> high bias? -> Y -> go back
   N -> Done.
   
*** High bias
   Training data problem, try
   - bigger network(more layers/units)
   - train longer
   - Maybe NN architecture search
*** High variance
   Dev set problem, try
   - more data
   - regularization
   - Maybe NN architecture search

  
* Regularization
   \lambda is the regularization parameter  
** Logistic regression
*** L2 Regularization
     + J(w, b) = $\frac{1}{m}$ $\sum_{i=1}^{m}$ L($\hat{y}^{i}$, y^{i}) + $\frac{\lambda}{2m}$ ||w||^{2}_{2}
     + ||w||^{2}_{2} = $\sum_{j=1}^{n}$ w_{j}^{2} = w^{T}w
*** L1 Regularization
     + $\frac{\lambda}{2m} \sum_{j=1}^{n}|w|_{1}$ = $\frac{\lambda}{2m}$ ||w||_{1}
   
** Neural Network
     + J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]}) = $\frac{1}{m}$ $\sum_{i=1}^{m}$ L($\hat{y}^{i}$, y^{i}) + $\frac{\lambda}{2m} \sum_{l=1}^{L}  \|w^{[l]}\|^{2}_{2}$
     + $\|w^{[l]}\|^{2}_{F}$ = $\sum_{i=1}^{n^{[l-1]}} \sum_{j=1}^{n^{[l]}} (w_{ij}^{[l]})^{2}$
       w: (n^{[l-1]}, n^{[l]})
       Frobenius norm
     + dw^{[l]} = (from backprop) + $\frac{\lambda}{m}$ w^{[l]}
       w^{[l]} := w^{[l]} - \alpha dw^{[l]}
                = w^{[l]} - \alpha [(from backprop) + $\frac{\lambda}{m}$ w^{[l]}]
                = w^{[l]} -  \alpha $\frac{\lambda}{m}$ w^{[l]} - \alpha (from backprop)
                = (1 - $\frac{\alpha\lambda}{m}$)w^{[l]} - \alpha (from backprop)

       L2 regrulariztion is sometimes called "Weight decay"

** How does regularization prevent overfitting?
*** Intuition 1
   Cranking up \lambda to be very big will set W close to 0 - you can think of it as zeroing out or at least reducing
   the impact of hidden units, so you end up with a much simpler network, as if you were just using logistic regression,
   which prevents overfitting.
*** Intuition 2
    For example, we are using tanh activation function.
    If \lambda is large, then W^{[l]} is small, then Z^{[l]} is small - it takes on a small range of values, g(z)
    would be roughly linear, as if every layer would be roughtly linear, as if it's just logistic regression,
    then the whole network is jsut a linera network, which prevents overfitting.



   
